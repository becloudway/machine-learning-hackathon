{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage for Sagemaker using the Kaggle Titanic dataset\n",
    "## Description of the dataset\n",
    "\n",
    "Variable|Definition|Key\n",
    ":---|:---|:---\n",
    "survival|Survival|0=No, 1=Yes\n",
    "pclass|Ticket class|1=1st, 2=2nd, 3=3rd\n",
    "sex|Sex|\n",
    "Age|Age in years|\n",
    "sibsp|# of siblings or spouses aboard the Titanic|\n",
    "parch|# of parents or children aboard the Titanic|\n",
    "ticket|Ticket number|\n",
    "fare|Passenger fare|\n",
    "cabin|Cabin number|\n",
    "embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "[More info on the dataset](https://www.kaggle.com/c/titanic/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "SOURCE_BUCKET = 'ml-hackathon-files'\n",
    "# Change X into team number\n",
    "BUCKET_NAME = 'ml-hackathon-teamX'\n",
    "\n",
    "def read_s3(key, bucketName=SOURCE_BUCKET, sep=\";\"):\n",
    "    client = boto3.client('s3') #low-level functional API\n",
    "    resource = boto3.resource('s3') #high-level object-oriented API\n",
    "    \n",
    "    obj = client.get_object(Bucket=bucketName, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj['Body'].read()), sep=sep)\n",
    "\n",
    "def write_csv(dataframe, prefix, key, bucket=BUCKET_NAME):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, index=False, encoding='utf-8', header=False)\n",
    "    csv_buffer.seek(0)\n",
    "    \n",
    "    csv_value = csv_buffer.getvalue()\n",
    "    csv_buffer.close()\n",
    "    \n",
    "    boto3.resource('s3').Bucket(bucket)\\\n",
    "        .Object(os.path.join(prefix, 'train', key))\\\n",
    "        .put(Body=csv_value, ContentType='text/csv')\n",
    "\n",
    "def write_recordio(X, y, prefix, key, bucket=BUCKET_NAME):    \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, X, y)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    boto3.resource('s3').Bucket(BUCKET_NAME).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "    s3_train_data = 's3://{}/{}/train/{}'.format(BUCKET_NAME, prefix, key)\n",
    "    print('uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "def set_categorical_columns(df, categorical_columns):\n",
    "    df = train_selection.copy()\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic = read_s3('titanic/train.csv', sep=',')\n",
    "print(titanic.shape) # Print the dimensions of the dataset\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "1. Select the columns that you would like to use\n",
    "2. convert into numpy arrays\n",
    "3. Write prepared data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Choose the variables you want to use in your model\n",
    "train_selection = titanic[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\"]].copy()\n",
    "train_selection = train_selection.dropna() # remove rows with missing values\n",
    "print(train_selection.shape)\n",
    "\n",
    "# Select outcome and predictor variables\n",
    "y = train_selection[\"Survived\"] # the value we want to predict\n",
    "X = train_selection.drop(\"Survived\", axis=1) # The values used to train the model for prediction\n",
    "\n",
    "# convert the data from the predictor variables into numerical arrays\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X = dv.fit_transform(X.to_dict(orient='records')) \n",
    "\n",
    "# RecordIO needs the input data to be floats\n",
    "y = np.where(np.array([t.tolist() for t in y]) == 0, 1, 0).astype('float32')\n",
    "X = np.where(np.array([t.tolist() for t in X]) == 0, 1, 0).astype('float32')\n",
    "\n",
    "# Write the preprocessed data back to S3\n",
    "prefix = \"titanic\"\n",
    "key = 'titanic'\n",
    "write_recordio(X, y, prefix, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Linear Learner model\n",
    "(the red output is expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(BUCKET_NAME, prefix)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(BUCKET_NAME, prefix, key)\n",
    "print(\"The model will be create using \" + s3_train_data)\n",
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest'}\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                       sagemaker.get_execution_role(), \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sagemaker.Session())\n",
    "\n",
    "feature_dimension = len(X[0])\n",
    "batch_size = 100\n",
    "linear.set_hyperparameters(feature_dim=feature_dimension,\n",
    "                           predictor_type='regressor',\n",
    "                           mini_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selection.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer\n",
    "\n",
    "predictions = [{'Pclass': 3, 'Sex': 'male', 'Age': 5, \"SibSp\": 1, \"Parch\":2, \"Embarked\": 'C'}, \n",
    "               {'Pclass': 3, 'Sex': 'female', 'Age': 7, \"SibSp\": 1, \"Parch\":2, \"Embarked\": 'C'}, \n",
    "               {'Pclass': 2, 'Sex': 'female', 'Age': 25, \"SibSp\": 0, \"Parch\":2, \"Embarked\": 'S'},\n",
    "               {'Pclass': 1, 'Sex': 'male', 'Age': 29, \"SibSp\": 0, \"Parch\":0, \"Embarked\": 'Q'}]\n",
    "transformed_predictions = dv.transform(predictions) # apply the transformations used in the training set\n",
    "\n",
    "linear_predictor.predict(transformed_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional) Delete the EndpointÂ¶\n",
    "#import sagemaker\n",
    "#sagemaker.Session().delete_endpoint(linear_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
